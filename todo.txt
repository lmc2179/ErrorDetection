- downsample training set (bayesian prior)
- equal probability for candidates in phrase table: 
- split set, put in dropbox
- test capitalization sensitivity in scorer
- word-based alignment files
- trim references from training set
- integrate m2scorer into tuning process instead of Bleu
- baseline DOCENT results
- randomize order to evenly split topic-based essays into dev/test/train buckets
- alignment file generator
- experiment with LM (baseline only uses training english for LM)
- experiment with augmented test (translated set of idiosyncratic errors)
- experiment with different training models for MOSES / different training parameters
- tokenize / stem input
